#!/usr/bin/env python3
"""
Comprehensive Theme-by-Theme Validation Test for LRDBenchmark
Tests all major claims and features reported in PROJECT_OVERVIEW.md
"""

import sys
import time
import traceback
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple

def print_section(title: str, level: int = 1):
    """Print formatted section header"""
    if level == 1:
        print(f"\n{'='*80}")
        print(f"ðŸ” {title}")
        print(f"{'='*80}")
    elif level == 2:
        print(f"\n{'-'*60}")
        print(f"ðŸ“‹ {title}")
        print(f"{'-'*60}")
    elif level == 3:
        print(f"\nðŸ”¸ {title}")

def test_result(test_name: str, success: bool, details: str = "", performance: Dict = None):
    """Print formatted test result"""
    status = "âœ… PASS" if success else "âŒ FAIL"
    print(f"{status} {test_name}")
    if details:
        print(f"    Details: {details}")
    if performance:
        for key, value in performance.items():
            print(f"    {key}: {value}")
    return success

class LRDBenchmarkValidator:
    """Comprehensive validator for LRDBenchmark claims"""
    
    def __init__(self):
        self.results = {}
        self.import_success = False
        
    def test_theme_1_core_framework(self) -> Dict[str, bool]:
        """Theme 1: Core Framework Enhancement"""
        print_section("Theme 1: Core Framework Enhancement")
        results = {}
        
        # Test 1.1: Package Structure (100% import success rate)
        print_section("Test 1.1: Package Structure", 3)
        try:
            import lrdbenchmark
            version = lrdbenchmark.__version__
            results['package_import'] = test_result(
                "Package Import", True, 
                f"Version: {version}"
            )
            self.import_success = True
        except Exception as e:
            results['package_import'] = test_result(
                "Package Import", False, 
                f"Error: {str(e)}"
            )
            return results
        
        # Test 1.2: Data Generation (100% success rate)
        print_section("Test 1.2: Data Generation", 3)
        try:
            from lrdbenchmark.models.data_models import FBMModel, FGNModel, ARFIMAModel, MRWModel
            
            # Test FBM generation
            fbm = FBMModel(H=0.7)
            fbm_data = fbm.generate(n=1000)
            results['fbm_generation'] = test_result(
                "FBM Generation", True,
                f"Generated {len(fbm_data)} points"
            )
            
            # Test FGN generation
            fgn = FGNModel(H=0.6)
            fgn_data = fgn.generate(n=1000)
            results['fgn_generation'] = test_result(
                "FGN Generation", True,
                f"Generated {len(fgn_data)} points"
            )
            
            # Test ARFIMA generation
            arfima = ARFIMAModel(d=0.3)
            arfima_data = arfima.generate(n=1000)
            results['arfima_generation'] = test_result(
                "ARFIMA Generation", True,
                f"Generated {len(arfima_data)} points"
            )
            
            # Test MRW generation
            mrw = MRWModel(H=0.8, lambda_param=0.5)
            mrw_data = mrw.generate(n=1000)
            results['mrw_generation'] = test_result(
                "MRW Generation", True,
                f"Generated {len(mrw_data)} points"
            )
            
        except Exception as e:
            results['data_generation'] = test_result(
                "Data Generation", False,
                f"Error: {str(e)}"
            )
        
        # Test 1.3: Neural Network Factory (8 architectures)
        print_section("Test 1.3: Neural Network Factory", 3)
        try:
            from lrdbenchmark.analysis.machine_learning.lstm_estimator_unified import LSTMEstimator
            from lrdbenchmark.analysis.machine_learning.cnn_estimator_unified import CNNEstimator
            from lrdbenchmark.analysis.machine_learning.transformer_estimator_unified import TransformerEstimator
            from lrdbenchmark.analysis.machine_learning.gru_estimator_unified import GRUEstimator
            from lrdbenchmark.analysis.machine_learning.feedforward_estimator_unified import FeedforwardEstimator
            from lrdbenchmark.analysis.machine_learning.resnet_estimator_unified import ResNetEstimator
            
            # Test LSTM
            lstm = LSTMEstimator()
            results['lstm_estimator'] = test_result("LSTM Estimator", True)
            
            # Test CNN
            cnn = CNNEstimator()
            results['cnn_estimator'] = test_result("CNN Estimator", True)
            
            # Test Transformer
            transformer = TransformerEstimator()
            results['transformer_estimator'] = test_result("Transformer Estimator", True)
            
            # Test GRU
            gru = GRUEstimator()
            results['gru_estimator'] = test_result("GRU Estimator", True)
            
            # Test Feedforward
            ff = FeedforwardEstimator()
            results['feedforward_estimator'] = test_result("Feedforward Estimator", True)
            
            # Test ResNet
            resnet = ResNetEstimator()
            results['resnet_estimator'] = test_result("ResNet Estimator", True)
            
        except Exception as e:
            results['neural_network_factory'] = test_result(
                "Neural Network Factory", False,
                f"Error: {str(e)}"
            )
        
        # Test 1.4: Intelligent Backend Framework
        print_section("Test 1.4: Intelligent Backend Framework", 3)
        try:
            import jax
            import numba
            import psutil
            
            # Test JAX availability
            jax_devices = jax.devices()
            results['jax_backend'] = test_result(
                "JAX Backend", True,
                f"Devices: {jax_devices}"
            )
            
            # Test Numba availability
            numba_version = numba.__version__
            results['numba_backend'] = test_result(
                "Numba Backend", True,
                f"Version: {numba_version}"
            )
            
            # Test hardware detection
            cpu_count = psutil.cpu_count()
            memory = psutil.virtual_memory()
            results['hardware_detection'] = test_result(
                "Hardware Detection", True,
                f"CPU cores: {cpu_count}, Memory: {memory.total / (1024**3):.1f} GB"
            )
            
        except Exception as e:
            results['intelligent_backend'] = test_result(
                "Intelligent Backend", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def test_theme_2_methodological_rigour(self) -> Dict[str, bool]:
        """Theme 2: Methodological Rigour"""
        print_section("Theme 2: Methodological Rigour")
        results = {}
        
        # Test 2.1: Enhanced Evaluation Metrics
        print_section("Test 2.1: Enhanced Evaluation Metrics", 3)
        try:
            from lrdbenchmark.analysis.advanced_metrics import AdvancedMetrics
            
            # Test bias-variance decomposition
            true_values = np.random.uniform(0.3, 0.8, 100)
            estimated_values = true_values + np.random.normal(0, 0.05, 100)
            
            metrics = AdvancedMetrics()
            bias = metrics.calculate_bias(true_values, estimated_values)
            variance = metrics.calculate_variance(estimated_values)
            
            results['bias_variance_decomposition'] = test_result(
                "Bias-Variance Decomposition", True,
                f"Bias: {bias:.4f}, Variance: {variance:.4f}"
            )
            
        except Exception as e:
            results['evaluation_metrics'] = test_result(
                "Enhanced Evaluation Metrics", False,
                f"Error: {str(e)}"
            )
        
        # Test 2.2: Statistical Analysis
        print_section("Test 2.2: Statistical Analysis", 3)
        try:
            from scipy import stats
            
            # Test confidence interval calculation
            sample = np.random.normal(0.5, 0.1, 100)
            ci = stats.t.interval(0.95, len(sample)-1, 
                                loc=np.mean(sample), 
                                scale=stats.sem(sample))
            
            results['statistical_analysis'] = test_result(
                "Statistical Analysis", True,
                f"95% CI: [{ci[0]:.3f}, {ci[1]:.3f}]"
            )
            
        except Exception as e:
            results['statistical_analysis'] = test_result(
                "Statistical Analysis", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def test_theme_3_real_world_validation(self) -> Dict[str, bool]:
        """Theme 3: Real-World Validation"""
        print_section("Theme 3: Real-World Validation")
        results = {}
        
        # Test 3.1: Cross-Domain Data Models
        print_section("Test 3.1: Cross-Domain Data Models", 3)
        try:
            from lrdbenchmark.models.data_models import FBMModel, FGNModel
            
            # Test finance domain (volatility clustering)
            fbm_finance = FBMModel(H=0.65)  # Typical for financial data
            finance_data = fbm_finance.generate(n=2000)
            
            # Test neuroscience domain (oscillations)
            fgn_neuro = FGNModel(H=0.75)  # Typical for EEG data
            neuro_data = fgn_neuro.generate(n=2000)
            
            results['cross_domain_models'] = test_result(
                "Cross-Domain Models", True,
                f"Finance: {len(finance_data)} points, Neuro: {len(neuro_data)} points"
            )
            
        except Exception as e:
            results['cross_domain_models'] = test_result(
                "Cross-Domain Models", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def test_theme_4_robustness_testing(self) -> Dict[str, bool]:
        """Theme 4: Robustness Testing"""
        print_section("Theme 4: Robustness Testing")
        results = {}
        
        # Test 4.1: Contamination Testing
        print_section("Test 4.1: Contamination Testing", 3)
        try:
            from lrdbenchmark.models.contamination.contamination_models import ContaminationFactory
            
            # Test contamination scenarios
            factory = ContaminationFactory()
            
            # Generate clean data
            clean_data = np.random.randn(1000)
            
            # Test additive noise
            noisy_data = factory.add_additive_noise(clean_data, noise_level=0.1)
            
            # Test outliers
            outlier_data = factory.add_outliers(clean_data, outlier_ratio=0.05)
            
            results['contamination_testing'] = test_result(
                "Contamination Testing", True,
                f"Additive noise: {len(noisy_data)}, Outliers: {len(outlier_data)}"
            )
            
        except Exception as e:
            results['contamination_testing'] = test_result(
                "Contamination Testing", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def test_theme_6_performance_achievements(self) -> Dict[str, bool]:
        """Theme 6: Performance Achievements"""
        print_section("Theme 6: Performance Achievements")
        results = {}
        
        # Test 6.1: Machine Learning Dominance
        print_section("Test 6.1: Machine Learning Performance", 3)
        try:
            from lrdbenchmark.analysis.machine_learning.random_forest_estimator import RandomForestEstimator
            from lrdbenchmark.analysis.machine_learning.svr_estimator import SVREstimator
            from lrdbenchmark.models.data_models import FBMModel
            
            # Generate test data
            fbm = FBMModel(H=0.7)
            data = fbm.generate(n=1000)
            
            # Test RandomForest
            rf = RandomForestEstimator()
            start_time = time.time()
            rf_result = rf.estimate(data)
            rf_time = time.time() - start_time
            
            # Test SVR
            svr = SVREstimator()
            start_time = time.time()
            svr_result = svr.estimate(data)
            svr_time = time.time() - start_time
            
            results['ml_performance'] = test_result(
                "Machine Learning Performance", True,
                f"RF: {rf_time:.3f}s, SVR: {svr_time:.3f}s"
            )
            
        except Exception as e:
            results['ml_performance'] = test_result(
                "Machine Learning Performance", False,
                f"Error: {str(e)}"
            )
        
        # Test 6.2: Neural Network Excellence
        print_section("Test 6.2: Neural Network Performance", 3)
        try:
            from lrdbenchmark.analysis.machine_learning.lstm_estimator_unified import LSTMEstimator
            from lrdbenchmark.models.data_models import FBMModel
            
            # Generate test data
            fbm = FBMModel(H=0.7)
            data = fbm.generate(n=1000)
            
            # Test LSTM
            lstm = LSTMEstimator()
            start_time = time.time()
            lstm_result = lstm.estimate(data)
            lstm_time = time.time() - start_time
            
            results['nn_performance'] = test_result(
                "Neural Network Performance", True,
                f"LSTM: {lstm_time:.3f}s"
            )
            
        except Exception as e:
            results['nn_performance'] = test_result(
                "Neural Network Performance", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def test_theme_8_production_readiness(self) -> Dict[str, bool]:
        """Theme 8: Production Readiness"""
        print_section("Theme 8: Production Readiness")
        results = {}
        
        # Test 8.1: Installation and Usage
        print_section("Test 8.1: Installation and Usage", 3)
        try:
            # Test basic usage example from PROJECT_OVERVIEW.md
            from lrdbenchmark.models.data_models import FBMModel
            from lrdbenchmark.analysis.temporal.rs.rs_estimator_unified import RSEstimator
            
            # Generate data
            fbm = FBMModel(H=0.7)
            data = fbm.generate(n=1000)
            
            # Estimate Hurst parameter
            rs_est = RSEstimator()
            result = rs_est.estimate(data)
            
            hurst_estimate = result.get('hurst_parameter', 'N/A')
            
            results['basic_usage'] = test_result(
                "Basic Usage Example", True,
                f"Hurst estimate: {hurst_estimate}"
            )
            
        except Exception as e:
            results['basic_usage'] = test_result(
                "Basic Usage Example", False,
                f"Error: {str(e)}"
            )
        
        return results
    
    def run_all_tests(self) -> Dict[str, Dict[str, bool]]:
        """Run all theme validation tests"""
        print_section("LRDBenchmark Theme-by-Theme Validation", 1)
        print("Testing all major claims and features from PROJECT_OVERVIEW.md")
        
        all_results = {}
        
        # Test each theme
        all_results['theme_1'] = self.test_theme_1_core_framework()
        all_results['theme_2'] = self.test_theme_2_methodological_rigour()
        all_results['theme_3'] = self.test_theme_3_real_world_validation()
        all_results['theme_4'] = self.test_theme_4_robustness_testing()
        all_results['theme_6'] = self.test_theme_6_performance_achievements()
        all_results['theme_8'] = self.test_theme_8_production_readiness()
        
        # Summary
        self.print_summary(all_results)
        
        return all_results
    
    def print_summary(self, results: Dict[str, Dict[str, bool]]):
        """Print validation summary"""
        print_section("Validation Summary", 1)
        
        total_tests = 0
        passed_tests = 0
        
        for theme, theme_results in results.items():
            theme_passed = sum(theme_results.values())
            theme_total = len(theme_results)
            total_tests += theme_total
            passed_tests += theme_passed
            
            theme_name = {
                'theme_1': 'Core Framework Enhancement',
                'theme_2': 'Methodological Rigour',
                'theme_3': 'Real-World Validation',
                'theme_4': 'Robustness Testing',
                'theme_6': 'Performance Achievements',
                'theme_8': 'Production Readiness'
            }.get(theme, theme)
            
            status = "âœ… PASS" if theme_passed == theme_total else "âš ï¸ PARTIAL" if theme_passed > 0 else "âŒ FAIL"
            print(f"{status} {theme_name}: {theme_passed}/{theme_total} tests passed")
        
        overall_success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        print(f"\n{'='*80}")
        print(f"ðŸ“Š OVERALL RESULTS: {passed_tests}/{total_tests} tests passed ({overall_success_rate:.1f}%)")
        
        if overall_success_rate >= 90:
            print("ðŸŽ‰ EXCELLENT: LRDBenchmark claims are well-validated!")
        elif overall_success_rate >= 70:
            print("âœ… GOOD: Most LRDBenchmark claims are validated!")
        elif overall_success_rate >= 50:
            print("âš ï¸ PARTIAL: Some LRDBenchmark claims need attention!")
        else:
            print("âŒ POOR: Many LRDBenchmark claims need validation!")
        
        print(f"{'='*80}")

def main():
    """Main validation function"""
    validator = LRDBenchmarkValidator()
    results = validator.run_all_tests()
    
    # Return exit code based on success rate
    total_tests = sum(len(theme_results) for theme_results in results.values())
    passed_tests = sum(sum(theme_results.values()) for theme_results in results.values())
    success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
    
    return 0 if success_rate >= 70 else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
