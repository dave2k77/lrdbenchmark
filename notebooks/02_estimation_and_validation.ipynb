{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation and Statistical Validation\n",
    "\n",
    "This notebook demonstrates the comprehensive estimation capabilities of the LRDBenchmark library, covering all available estimator categories with statistical validation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Long-range dependence estimation is a critical task in time series analysis. This notebook covers:\n",
    "\n",
    "1. **Estimator Categories**: Classical, Machine Learning, and Neural Network estimators\n",
    "2. **Statistical Validation**: Confidence intervals, bootstrap methods, convergence analysis\n",
    "3. **Performance Comparison**: Accuracy, speed, and robustness across different estimators\n",
    "4. **Decision Guidelines**: When to use which estimator\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Estimator Categories Overview](#overview)\n",
    "3. [Classical Estimators](#classical)\n",
    "4. [Machine Learning Estimators](#ml)\n",
    "5. [Neural Network Estimators](#neural)\n",
    "6. [Statistical Validation](#validation)\n",
    "7. [Performance Comparison](#comparison)\n",
    "8. [Decision Guidelines](#guidelines)\n",
    "9. [Summary and Next Steps](#summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports {#setup}\n",
    "\n",
    "First, let's import all necessary libraries and set up the environment for reproducible results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific computing imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import bootstrap\n",
    "import time\n",
    "import warnings\n",
    "import subprocess\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set JAX to use CPU to avoid CUDA issues\n",
    "import os\n",
    "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
    "\n",
    "# GPU Memory Management Functions\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check current GPU memory usage.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        used, total = map(int, result.stdout.strip().split(', '))\n",
    "        usage_percent = (used / total) * 100\n",
    "        print(f\"üñ•Ô∏è  GPU Memory: {used}MB / {total}MB ({usage_percent:.1f}%)\")\n",
    "        if usage_percent > 80:\n",
    "            print(\"‚ö†Ô∏è  Warning: GPU memory usage is high!\")\n",
    "        return used, total, usage_percent\n",
    "    except:\n",
    "        print(\"‚ÑπÔ∏è  Could not check GPU memory (nvidia-smi not available)\")\n",
    "        return None, None, None\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory cache.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            print(\"üßπ GPU memory cache cleared\")\n",
    "    except ImportError:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    print(\"üßπ System memory garbage collected\")\n",
    "\n",
    "# Check initial GPU status\n",
    "print(\"üîç Checking GPU memory status...\")\n",
    "check_gpu_memory()\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import LRDBenchmark data models\n",
    "from lrdbenchmark.models.data_models.fbm.fbm_model import FractionalBrownianMotion\n",
    "from lrdbenchmark.models.data_models.fgn.fgn_model import FractionalGaussianNoise\n",
    "\n",
    "# Import Classical estimators\n",
    "from lrdbenchmark.analysis.temporal.rs.rs_estimator_unified import RSEstimator\n",
    "from lrdbenchmark.analysis.temporal.dfa.dfa_estimator_unified import DFAEstimator\n",
    "from lrdbenchmark.analysis.temporal.dma.dma_estimator_unified import DMAEstimator\n",
    "from lrdbenchmark.analysis.temporal.higuchi.higuchi_estimator_unified import HiguchiEstimator\n",
    "from lrdbenchmark.analysis.spectral.gph.gph_estimator_unified import GPHEstimator\n",
    "from lrdbenchmark.analysis.spectral.whittle.whittle_estimator_unified import WhittleEstimator\n",
    "from lrdbenchmark.analysis.spectral.periodogram.periodogram_estimator_unified import PeriodogramEstimator\n",
    "\n",
    "# Import ML estimators\n",
    "from lrdbenchmark.analysis.machine_learning.random_forest_estimator_unified import RandomForestEstimator\n",
    "from lrdbenchmark.analysis.machine_learning.svr_estimator_unified import SVREstimator\n",
    "from lrdbenchmark.analysis.machine_learning.gradient_boosting_estimator_unified import GradientBoostingEstimator\n",
    "\n",
    "# Import Neural Network estimators\n",
    "from lrdbenchmark.analysis.machine_learning.cnn_estimator_unified import CNNEstimator\n",
    "from lrdbenchmark.analysis.machine_learning.lstm_estimator_unified import LSTMEstimator\n",
    "from lrdbenchmark.analysis.machine_learning.gru_estimator_unified import GRUEstimator\n",
    "from lrdbenchmark.analysis.machine_learning.transformer_estimator_unified import TransformerEstimator\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estimator Categories Overview {#overview}\n",
    "\n",
    "LRDBenchmark provides three main categories of Hurst parameter estimators:\n",
    "\n",
    "### 1. Classical Estimators\n",
    "- **Temporal**: R/S Analysis, DFA, DMA, Higuchi\n",
    "- **Spectral**: GPH, Whittle, Periodogram\n",
    "- **Wavelet**: CWT, Wavelet Variance, Log Variance, Wavelet Whittle\n",
    "- **Multifractal**: MFDFA, Wavelet Leaders\n",
    "\n",
    "### 2. Machine Learning Estimators\n",
    "- **Random Forest**: Ensemble tree-based estimation\n",
    "- **Support Vector Regression**: SVM-based estimation\n",
    "- **Gradient Boosting**: Boosted tree estimation\n",
    "\n",
    "### 3. Neural Network Estimators\n",
    "- **CNN**: Convolutional Neural Networks\n",
    "- **LSTM**: Long Short-Term Memory networks\n",
    "- **GRU**: Gated Recurrent Units\n",
    "- **Transformer**: Attention-based architectures\n",
    "\n",
    "Let's demonstrate each category with comprehensive examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical Estimators {#classical}\n",
    "\n",
    "Classical estimators are based on well-established statistical methods for LRD estimation. They are fast, interpretable, and have strong theoretical foundations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data with known Hurst parameters\n",
    "print(\"üîç Generating test data for classical estimator evaluation...\")\n",
    "\n",
    "# Test with different Hurst parameters\n",
    "H_values = [0.3, 0.5, 0.7, 0.9]\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate FBM data for each H value\n",
    "test_data = {}\n",
    "for H in H_values:\n",
    "    fbm = FractionalBrownianMotion(H=H, sigma=1.0)\n",
    "    data = fbm.generate(n_samples, seed=42)\n",
    "    test_data[f'H={H}'] = {'data': data, 'true_H': H}\n",
    "\n",
    "print(f\"Generated {len(test_data)} test datasets\")\n",
    "\n",
    "# Initialize classical estimators\n",
    "classical_estimators = {\n",
    "    'R/S Analysis': RSEstimator(),\n",
    "    'DFA': DFAEstimator(),\n",
    "    'DMA': DMAEstimator(),\n",
    "    'Higuchi': HiguchiEstimator(),\n",
    "    'GPH': GPHEstimator(),\n",
    "    'Whittle': WhittleEstimator(),\n",
    "    'Periodogram': PeriodogramEstimator()\n",
    "}\n",
    "\n",
    "print(f\"Initialized {len(classical_estimators)} classical estimators\")\n",
    "\n",
    "# Test classical estimators\n",
    "print(\"\\nüìä Classical Estimator Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for data_name, data_info in test_data.items():\n",
    "    data = data_info['data']\n",
    "    true_H = data_info['true_H']\n",
    "    \n",
    "    print(f\"\\n{data_name} (True H = {true_H}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for estimator_name, estimator in classical_estimators.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = estimator.estimate(data)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if isinstance(result, dict):\n",
    "                estimated_H = result.get('hurst_parameter', result.get('H', None))\n",
    "            else:\n",
    "                estimated_H = result\n",
    "            \n",
    "            if estimated_H is not None:\n",
    "                error = abs(estimated_H - true_H)\n",
    "                execution_time = end_time - start_time\n",
    "                \n",
    "                print(f\"  {estimator_name:12}: H = {estimated_H:.4f}, Error = {error:.4f}, Time = {execution_time:.3f}s\")\n",
    "                \n",
    "                results.append({\n",
    "                    'Data': data_name,\n",
    "                    'True_H': true_H,\n",
    "                    'Estimator': estimator_name,\n",
    "                    'Estimated_H': estimated_H,\n",
    "                    'Error': error,\n",
    "                    'Execution_Time': execution_time,\n",
    "                    'Category': 'Classical'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  {estimator_name:12}: Failed to estimate\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {estimator_name:12}: Error - {str(e)[:50]}...\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nüìà Summary: {len(results_df)} successful estimations\")\n",
    "\n",
    "# Calculate performance metrics\n",
    "if len(results_df) > 0:\n",
    "    performance_summary = results_df.groupby('Estimator').agg({\n",
    "        'Error': ['mean', 'std', 'min', 'max'],\n",
    "        'Execution_Time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"\\nüìä Performance Summary (Classical Estimators):\")\n",
    "    print(performance_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Estimators {#ml}\n",
    "\n",
    "Machine Learning estimators use pre-trained models to estimate Hurst parameters. They are particularly useful for complex time series patterns and can handle non-standard LRD processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ML estimators\n",
    "print(\"\\nü§ñ Machine Learning Estimator Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ml_estimators = {\n",
    "    'Random Forest': RandomForestEstimator(),\n",
    "    'SVR': SVREstimator(),\n",
    "    'Gradient Boosting': GradientBoostingEstimator()\n",
    "}\n",
    "\n",
    "for data_name, data_info in test_data.items():\n",
    "    data = data_info['data']\n",
    "    true_H = data_info['true_H']\n",
    "    \n",
    "    print(f\"\\n{data_name} (True H = {true_H}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for estimator_name, estimator in ml_estimators.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = estimator.estimate(data)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if isinstance(result, dict):\n",
    "                estimated_H = result.get('hurst_parameter', result.get('H', None))\n",
    "            else:\n",
    "                estimated_H = result\n",
    "            \n",
    "            if estimated_H is not None:\n",
    "                error = abs(estimated_H - true_H)\n",
    "                execution_time = end_time - start_time\n",
    "                \n",
    "                print(f\"  {estimator_name:15}: H = {estimated_H:.4f}, Error = {error:.4f}, Time = {execution_time:.3f}s\")\n",
    "                \n",
    "                results.append({\n",
    "                    'Data': data_name,\n",
    "                    'True_H': true_H,\n",
    "                    'Estimator': estimator_name,\n",
    "                    'Estimated_H': estimated_H,\n",
    "                    'Error': error,\n",
    "                    'Execution_Time': execution_time,\n",
    "                    'Category': 'ML'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  {estimator_name:15}: Failed to estimate\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {estimator_name:15}: Error - {str(e)[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Estimators {#neural}\n",
    "\n",
    "Neural Network estimators use deep learning models to estimate Hurst parameters. They can capture complex non-linear patterns and are particularly effective for high-dimensional time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory Management for Neural Networks\n",
    "print(\"\\nüîß GPU Memory Management:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check GPU memory before neural network operations\n",
    "print(\"üîç Checking GPU memory before neural network operations...\")\n",
    "check_gpu_memory()\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "print(\"\\nüßπ Clearing GPU memory...\")\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Check GPU memory after cleanup\n",
    "print(\"\\nüîç Checking GPU memory after cleanup...\")\n",
    "check_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Neural Network estimators\n",
    "print(\"\\nüß† Neural Network Estimator Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "neural_estimators = {\n",
    "    'CNN': CNNEstimator(),\n",
    "    'LSTM': LSTMEstimator(),\n",
    "    'GRU': GRUEstimator(),\n",
    "    'Transformer': TransformerEstimator()\n",
    "}\n",
    "\n",
    "for data_name, data_info in test_data.items():\n",
    "    data = data_info['data']\n",
    "    true_H = data_info['true_H']\n",
    "    \n",
    "    print(f\"\\n{data_name} (True H = {true_H}):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for estimator_name, estimator in neural_estimators.items():\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = estimator.estimate(data)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            if isinstance(result, dict):\n",
    "                estimated_H = result.get('hurst_parameter', result.get('H', None))\n",
    "            else:\n",
    "                estimated_H = result\n",
    "            \n",
    "            if estimated_H is not None:\n",
    "                error = abs(estimated_H - true_H)\n",
    "                execution_time = end_time - start_time\n",
    "                \n",
    "                print(f\"  {estimator_name:12}: H = {estimated_H:.4f}, Error = {error:.4f}, Time = {execution_time:.3f}s\")\n",
    "                \n",
    "                results.append({\n",
    "                    'Data': data_name,\n",
    "                    'True_H': true_H,\n",
    "                    'Estimator': estimator_name,\n",
    "                    'Estimated_H': estimated_H,\n",
    "                    'Error': error,\n",
    "                    'Execution_Time': execution_time,\n",
    "                    'Category': 'Neural'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"  {estimator_name:12}: Failed to estimate\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {estimator_name:12}: Error - {str(e)[:50]}...\")\n",
    "\n",
    "# Update results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nüìà Total successful estimations: {len(results_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison {#comparison}\n",
    "\n",
    "Let's create comprehensive visualizations comparing all estimator categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance comparison\n",
    "if len(results_df) > 0:\n",
    "    print(\"üìä Creating performance comparison visualizations...\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Error distribution by category\n",
    "    ax1 = axes[0, 0]\n",
    "    for category in results_df['Category'].unique():\n",
    "        category_data = results_df[results_df['Category'] == category]['Error']\n",
    "        ax1.hist(category_data, alpha=0.7, label=category, bins=15)\n",
    "    ax1.set_xlabel('Absolute Error')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Error Distribution by Category')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Execution time by category\n",
    "    ax2 = axes[0, 1]\n",
    "    for category in results_df['Category'].unique():\n",
    "        category_data = results_df[results_df['Category'] == category]['Execution_Time']\n",
    "        ax2.hist(category_data, alpha=0.7, label=category, bins=15)\n",
    "    ax2.set_xlabel('Execution Time (seconds)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Execution Time Distribution by Category')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Error vs True H\n",
    "    ax3 = axes[1, 0]\n",
    "    for category in results_df['Category'].unique():\n",
    "        category_data = results_df[results_df['Category'] == category]\n",
    "        ax3.scatter(category_data['True_H'], category_data['Error'], \n",
    "                   alpha=0.7, label=category, s=50)\n",
    "    ax3.set_xlabel('True Hurst Parameter')\n",
    "    ax3.set_ylabel('Absolute Error')\n",
    "    ax3.set_title('Error vs True Hurst Parameter')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Performance summary by estimator\n",
    "    ax4 = axes[1, 1]\n",
    "    performance_by_estimator = results_df.groupby('Estimator')['Error'].mean().sort_values()\n",
    "    ax4.bar(range(len(performance_by_estimator)), performance_by_estimator.values, alpha=0.7)\n",
    "    ax4.set_xlabel('Estimator')\n",
    "    ax4.set_ylabel('Mean Absolute Error')\n",
    "    ax4.set_title('Mean Error by Estimator')\n",
    "    ax4.set_xticks(range(len(performance_by_estimator)))\n",
    "    ax4.set_xticklabels(performance_by_estimator.index, rotation=45, ha='right')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/estimator_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance summary table\n",
    "    print(\"\\nüìä Performance Summary by Category:\")\n",
    "    category_summary = results_df.groupby('Category').agg({\n",
    "        'Error': ['mean', 'std', 'min', 'max'],\n",
    "        'Execution_Time': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    print(category_summary)\n",
    "    \n",
    "    # Best performing estimators\n",
    "    print(\"\\nüèÜ Top 5 Best Performing Estimators (by mean error):\")\n",
    "    best_estimators = results_df.groupby('Estimator')['Error'].mean().sort_values().head()\n",
    "    for i, (estimator, error) in enumerate(best_estimators.items(), 1):\n",
    "        print(f\"  {i}. {estimator}: {error:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('outputs/estimator_results.csv', index=False)\n",
    "    print(\"\\nüíæ Results saved to outputs/estimator_results.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No successful estimations to compare\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Guidelines {#guidelines}\n",
    "\n",
    "### When to Use Which Estimator\n",
    "\n",
    "#### Classical Estimators\n",
    "- **Best for**: Standard LRD processes, interpretable results, fast computation\n",
    "- **Use when**: You need theoretical guarantees, have clean data, want fast results\n",
    "- **Recommended**: R/S Analysis, DFA, GPH for most applications\n",
    "\n",
    "#### Machine Learning Estimators\n",
    "- **Best for**: Complex patterns, non-standard LRD processes, pre-trained models\n",
    "- **Use when**: You have diverse data types, need robust estimation, have computational resources\n",
    "- **Recommended**: Random Forest for general use, SVR for smooth patterns\n",
    "\n",
    "#### Neural Network Estimators\n",
    "- **Best for**: High-dimensional data, complex non-linear patterns, large datasets\n",
    "- **Use when**: You have sufficient data, need state-of-the-art accuracy, can afford training time\n",
    "- **Recommended**: CNN for spatial patterns, LSTM for temporal sequences, Transformer for attention-based patterns\n",
    "\n",
    "### Performance Trade-offs\n",
    "\n",
    "1. **Accuracy vs Speed**: Classical < ML < Neural (generally)\n",
    "2. **Interpretability**: Classical > ML > Neural\n",
    "3. **Robustness**: Depends on data quality and estimator choice\n",
    "4. **Computational Requirements**: Classical < ML < Neural\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps {#summary}\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Estimator Diversity**: LRDBenchmark provides comprehensive coverage across three categories:\n",
    "   - **Classical**: Fast, interpretable, theoretically grounded\n",
    "   - **Machine Learning**: Robust, flexible, pre-trained models\n",
    "   - **Neural Networks**: High accuracy, complex patterns, state-of-the-art\n",
    "\n",
    "2. **Performance Characteristics**:\n",
    "   - Classical estimators are fastest and most interpretable\n",
    "   - ML estimators provide good balance of accuracy and robustness\n",
    "   - Neural networks offer highest accuracy for complex patterns\n",
    "\n",
    "3. **Selection Guidelines**:\n",
    "   - Use classical estimators for standard LRD analysis\n",
    "   - Use ML estimators for diverse data types and robustness\n",
    "   - Use neural networks for complex patterns and high accuracy requirements\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Benchmarking**: Compare estimators systematically across different data types\n",
    "2. **Custom Estimators**: Learn how to extend the library with custom estimators\n",
    "3. **Real-world Application**: Apply estimators to actual time series data\n",
    "4. **Performance Optimization**: Explore advanced optimization techniques\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `outputs/estimator_performance_comparison.png`: Comprehensive performance visualization\n",
    "- `outputs/estimator_results.csv`: Detailed results table\n",
    "- Performance metrics and rankings\n",
    "\n",
    "### References\n",
    "\n",
    "1. Hurst, H. E. (1951). Long-term storage capacity of reservoirs. Transactions of the American Society of Civil Engineers, 116(1), 770-808.\n",
    "2. Peng, C. K., et al. (1994). Mosaic organization of DNA nucleotides. Physical review E, 49(2), 1685.\n",
    "3. Geweke, J., & Porter-Hudak, S. (1983). The estimation and application of long memory time series models. Journal of time series analysis, 4(4), 221-238.\n",
    "4. Abry, P., & Veitch, D. (1998). Wavelet analysis of long-range-dependent traffic. IEEE Transactions on information theory, 44(1), 2-15.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Notebook**: [03_custom_models_and_estimators.ipynb](03_custom_models_and_estimators.ipynb) - Learn how to extend the library with custom data models and estimators.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
