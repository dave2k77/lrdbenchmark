{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Leaderboard Generation\n",
        "\n",
        "This notebook demonstrates how to create comprehensive performance leaderboards from benchmark results, showing how to rank estimators and generate publication-ready comparisons.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The leaderboard generation system allows you to:\n",
        "\n",
        "1. **Load Benchmark Results**: Import results from multiple benchmark runs\n",
        "2. **Create Rankings**: Generate performance rankings across different metrics\n",
        "3. **Composite Scoring**: Combine multiple metrics into overall scores\n",
        "4. **Visualization**: Create publication-ready plots and tables\n",
        "5. **Export Results**: Save leaderboards in various formats\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Loading Benchmark Results](#loading)\n",
        "3. [Creating Performance Rankings](#rankings)\n",
        "4. [Composite Scoring System](#scoring)\n",
        "5. [Visualization and Export](#visualization)\n",
        "6. [Summary and Next Steps](#summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports {#setup}\n",
        "\n",
        "First, let's import all necessary libraries and set up the leaderboard generation system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard scientific computing imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Import LRDBenchmark leaderboard system\n",
        "from lrdbenchmark.analysis.benchmark import ComprehensiveBenchmark\n",
        "\n",
        "# Import data models for generating test data\n",
        "from lrdbenchmark.models.data_models.fbm.fbm_model import FractionalBrownianMotion\n",
        "from lrdbenchmark.models.data_models.fgn.fgn_model import FractionalGaussianNoise\n",
        "\n",
        "# Import estimators for testing\n",
        "from lrdbenchmark.analysis.temporal.rs.rs_estimator_unified import RSEstimator\n",
        "from lrdbenchmark.analysis.temporal.dfa.dfa_estimator_unified import DFAEstimator\n",
        "from lrdbenchmark.analysis.spectral.gph.gph_estimator_unified import GPHEstimator\n",
        "from lrdbenchmark.analysis.spectral.whittle.whittle_estimator_unified import WhittleEstimator\n",
        "from lrdbenchmark.analysis.machine_learning.random_forest_estimator_unified import RandomForestEstimator\n",
        "from lrdbenchmark.analysis.machine_learning.svr_estimator_unified import SVREstimator\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(\"üèÜ Ready to generate performance leaderboards\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading Benchmark Results {#loading}\n",
        "\n",
        "Let's run comprehensive benchmarks to generate data for our leaderboard, then load and process the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize benchmark system\n",
        "print(\"üîß Initializing Benchmark System for Leaderboard Generation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "benchmark = ComprehensiveBenchmark(output_dir=\"leaderboard_results\")\n",
        "\n",
        "# Run comprehensive benchmarks\n",
        "print(\"\\nüöÄ Running Comprehensive Benchmarks...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Run classical benchmark\n",
        "print(\"üìä Running Classical Estimator Benchmark...\")\n",
        "classical_results = benchmark.run_classical_benchmark(\n",
        "    data_length=1000,\n",
        "    n_runs=10,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Classical benchmark completed!\")\n",
        "print(f\"Success rate: {classical_results['success_rate']:.1%}\")\n",
        "print(f\"Total tests: {classical_results['total_tests']}\")\n",
        "\n",
        "# Run ML benchmark\n",
        "print(\"\\nüìä Running ML Estimator Benchmark...\")\n",
        "ml_results = benchmark.run_ml_benchmark(\n",
        "    data_length=1000,\n",
        "    n_runs=5,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ ML benchmark completed!\")\n",
        "print(f\"Success rate: {ml_results['success_rate']:.1%}\")\n",
        "print(f\"Total tests: {ml_results['total_tests']}\")\n",
        "\n",
        "# Run neural benchmark\n",
        "print(\"\\nüìä Running Neural Network Benchmark...\")\n",
        "neural_results = benchmark.run_neural_benchmark(\n",
        "    data_length=1000,\n",
        "    n_runs=3,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Neural benchmark completed!\")\n",
        "print(f\"Success rate: {neural_results['success_rate']:.1%}\")\n",
        "print(f\"Total tests: {neural_results['total_tests']}\")\n",
        "\n",
        "# Run comprehensive benchmark\n",
        "print(\"\\nüìä Running Comprehensive Benchmark...\")\n",
        "comprehensive_results = benchmark.run_comprehensive_benchmark(\n",
        "    data_length=1000,\n",
        "    n_runs=5,\n",
        "    save_results=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Comprehensive benchmark completed!\")\n",
        "print(f\"Success rate: {comprehensive_results['success_rate']:.1%}\")\n",
        "print(f\"Total tests: {comprehensive_results['total_tests']}\")\n",
        "\n",
        "print(\"\\nüéØ All benchmarks completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creating Performance Rankings {#rankings}\n",
        "\n",
        "Now let's create comprehensive performance rankings and leaderboards from our benchmark results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive leaderboard\n",
        "print(\"üèÜ Creating Performance Leaderboard...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Combine all benchmark results\n",
        "all_results = {\n",
        "    'Classical': classical_results,\n",
        "    'ML': ml_results,\n",
        "    'Neural': neural_results,\n",
        "    'Comprehensive': comprehensive_results\n",
        "}\n",
        "\n",
        "# Create performance summary\n",
        "performance_data = []\n",
        "\n",
        "for category, results in all_results.items():\n",
        "    if 'summary' in results and 'estimator_results' in results['summary']:\n",
        "        for estimator_result in results['summary']['estimator_results']:\n",
        "            if estimator_result['success']:\n",
        "                performance_data.append({\n",
        "                    'Category': category,\n",
        "                    'Estimator': estimator_result['estimator'],\n",
        "                    'True_H': estimator_result['true_hurst'],\n",
        "                    'Estimated_H': estimator_result['estimated_hurst'],\n",
        "                    'Error': estimator_result['error'],\n",
        "                    'Execution_Time': estimator_result['execution_time'],\n",
        "                    'Data_Model': estimator_result.get('data_model', 'Unknown')\n",
        "                })\n",
        "\n",
        "# Create DataFrame\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "if len(performance_df) > 0:\n",
        "    print(f\"üìä Loaded {len(performance_df)} performance records\")\n",
        "    \n",
        "    # Calculate performance metrics\n",
        "    performance_metrics = performance_df.groupby(['Category', 'Estimator']).agg({\n",
        "        'Error': ['mean', 'std', 'min', 'max'],\n",
        "        'Execution_Time': ['mean', 'std'],\n",
        "        'True_H': 'count'\n",
        "    }).round(4)\n",
        "    \n",
        "    print(\"\\nüìà Performance Metrics Summary:\")\n",
        "    print(performance_metrics)\n",
        "    \n",
        "    # Create overall leaderboard\n",
        "    print(\"\\nüèÜ Overall Performance Leaderboard:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Calculate composite scores\n",
        "    leaderboard_data = []\n",
        "    \n",
        "    for (category, estimator), group in performance_df.groupby(['Category', 'Estimator']):\n",
        "        mean_error = group['Error'].mean()\n",
        "        std_error = group['Error'].std()\n",
        "        mean_time = group['Execution_Time'].mean()\n",
        "        count = len(group)\n",
        "        \n",
        "        # Composite score (lower is better for error, higher is better for count)\n",
        "        composite_score = (1 / (1 + mean_error)) * (count / 10) * (1 / (1 + mean_time))\n",
        "        \n",
        "        leaderboard_data.append({\n",
        "            'Category': category,\n",
        "            'Estimator': estimator,\n",
        "            'Mean_Error': mean_error,\n",
        "            'Std_Error': std_error,\n",
        "            'Mean_Time': mean_time,\n",
        "            'Count': count,\n",
        "            'Composite_Score': composite_score\n",
        "        })\n",
        "    \n",
        "    leaderboard_df = pd.DataFrame(leaderboard_data)\n",
        "    leaderboard_df = leaderboard_df.sort_values('Composite_Score', ascending=False)\n",
        "    \n",
        "    print(leaderboard_df.round(4))\n",
        "    \n",
        "    # Save leaderboard\n",
        "    leaderboard_df.to_csv('outputs/performance_leaderboard.csv', index=False)\n",
        "    print(\"\\nüíæ Leaderboard saved to outputs/performance_leaderboard.csv\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No performance data available for leaderboard generation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualization and Export {#visualization}\n",
        "\n",
        "Let's create comprehensive visualizations of our leaderboard results and export them in various formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "if len(performance_df) > 0:\n",
        "    print(\"üìä Creating Performance Visualizations...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "    \n",
        "    # 1. Error distribution by category\n",
        "    ax1 = axes[0, 0]\n",
        "    for category in performance_df['Category'].unique():\n",
        "        category_data = performance_df[performance_df['Category'] == category]['Error']\n",
        "        ax1.hist(category_data, alpha=0.7, label=category, bins=15)\n",
        "    ax1.set_xlabel('Absolute Error')\n",
        "    ax1.set_ylabel('Frequency')\n",
        "    ax1.set_title('Error Distribution by Category')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Execution time by category\n",
        "    ax2 = axes[0, 1]\n",
        "    for category in performance_df['Category'].unique():\n",
        "        category_data = performance_df[performance_df['Category'] == category]['Execution_Time']\n",
        "        ax2.hist(category_data, alpha=0.7, label=category, bins=15)\n",
        "    ax2.set_xlabel('Execution Time (seconds)')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Execution Time Distribution by Category')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Error vs True H\n",
        "    ax3 = axes[0, 2]\n",
        "    for category in performance_df['Category'].unique():\n",
        "        category_data = performance_df[performance_df['Category'] == category]\n",
        "        ax3.scatter(category_data['True_H'], category_data['Error'], \n",
        "                   alpha=0.7, label=category, s=50)\n",
        "    ax3.set_xlabel('True Hurst Parameter')\n",
        "    ax3.set_ylabel('Absolute Error')\n",
        "    ax3.set_title('Error vs True Hurst Parameter')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Performance by estimator\n",
        "    ax4 = axes[1, 0]\n",
        "    estimator_performance = performance_df.groupby('Estimator')['Error'].mean().sort_values()\n",
        "    ax4.bar(range(len(estimator_performance)), estimator_performance.values, alpha=0.7)\n",
        "    ax4.set_xlabel('Estimator')\n",
        "    ax4.set_ylabel('Mean Absolute Error')\n",
        "    ax4.set_title('Mean Error by Estimator')\n",
        "    ax4.set_xticks(range(len(estimator_performance)))\n",
        "    ax4.set_xticklabels(estimator_performance.index, rotation=45, ha='right')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Execution time by estimator\n",
        "    ax5 = axes[1, 1]\n",
        "    time_performance = performance_df.groupby('Estimator')['Execution_Time'].mean().sort_values()\n",
        "    ax5.bar(range(len(time_performance)), time_performance.values, alpha=0.7)\n",
        "    ax5.set_xlabel('Estimator')\n",
        "    ax5.set_ylabel('Mean Execution Time (seconds)')\n",
        "    ax5.set_title('Mean Execution Time by Estimator')\n",
        "    ax5.set_xticks(range(len(time_performance)))\n",
        "    ax5.set_xticklabels(time_performance.index, rotation=45, ha='right')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Composite score ranking\n",
        "    ax6 = axes[1, 2]\n",
        "    if len(leaderboard_df) > 0:\n",
        "        top_10 = leaderboard_df.head(10)\n",
        "        ax6.barh(range(len(top_10)), top_10['Composite_Score'], alpha=0.7)\n",
        "        ax6.set_xlabel('Composite Score')\n",
        "        ax6.set_ylabel('Rank')\n",
        "        ax6.set_title('Top 10 Estimators by Composite Score')\n",
        "        ax6.set_yticks(range(len(top_10)))\n",
        "        ax6.set_yticklabels([f\"{row['Category']} - {row['Estimator']}\" for _, row in top_10.iterrows()])\n",
        "        ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/leaderboard_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # Create category-specific leaderboards\n",
        "    print(\"\\nüìä Category-Specific Leaderboards:\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for category in performance_df['Category'].unique():\n",
        "        category_data = performance_df[performance_df['Category'] == category]\n",
        "        category_leaderboard = category_data.groupby('Estimator').agg({\n",
        "            'Error': ['mean', 'std'],\n",
        "            'Execution_Time': 'mean',\n",
        "            'True_H': 'count'\n",
        "        }).round(4)\n",
        "        \n",
        "        print(f\"\\n{category} Category Leaderboard:\")\n",
        "        print(category_leaderboard)\n",
        "    \n",
        "    # Export results in multiple formats\n",
        "    print(\"\\nüíæ Exporting Results...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # CSV export\n",
        "    performance_df.to_csv('outputs/performance_data.csv', index=False)\n",
        "    print(\"‚úÖ Performance data exported to CSV\")\n",
        "    \n",
        "    # JSON export\n",
        "    performance_df.to_json('outputs/performance_data.json', orient='records', indent=2)\n",
        "    print(\"‚úÖ Performance data exported to JSON\")\n",
        "    \n",
        "    # LaTeX table export\n",
        "    if len(leaderboard_df) > 0:\n",
        "        latex_table = leaderboard_df.to_latex(index=False, float_format='%.4f')\n",
        "        with open('outputs/leaderboard_table.tex', 'w') as f:\n",
        "            f.write(latex_table)\n",
        "        print(\"‚úÖ Leaderboard table exported to LaTeX\")\n",
        "    \n",
        "    print(\"\\nüéØ All visualizations and exports completed successfully!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No performance data available for visualization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps {#summary}\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Leaderboard Generation**: LRDBenchmark provides comprehensive tools for creating performance leaderboards:\n",
        "   - **Multi-category Comparison**: Classical, ML, and Neural estimators\n",
        "   - **Composite Scoring**: Combined accuracy, speed, and reliability metrics\n",
        "   - **Statistical Analysis**: Confidence intervals and significance tests\n",
        "   - **Publication-ready Output**: LaTeX, CSV, JSON formats\n",
        "\n",
        "2. **Performance Rankings**: The system generates multiple types of leaderboards:\n",
        "   - **Overall Leaderboard**: Combined performance across all categories\n",
        "   - **Category-specific**: Rankings within each estimator category\n",
        "   - **Metric-specific**: Rankings by accuracy, speed, or reliability\n",
        "   - **Composite Scoring**: Weighted combination of multiple metrics\n",
        "\n",
        "3. **Visualization**: Comprehensive plots and tables for:\n",
        "   - **Error Distributions**: Performance across different scenarios\n",
        "   - **Execution Time Analysis**: Computational efficiency comparison\n",
        "   - **Scatter Plots**: Error vs true Hurst parameter relationships\n",
        "   - **Bar Charts**: Direct performance comparisons\n",
        "\n",
        "### Leaderboard Results\n",
        "\n",
        "- **Top Performers**: Best estimators across different categories\n",
        "- **Performance Trade-offs**: Accuracy vs speed analysis\n",
        "- **Category Strengths**: Each category's optimal use cases\n",
        "- **Statistical Significance**: Confidence in performance differences\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Real-world Application**: Apply leaderboards to actual time series data\n",
        "2. **Advanced Analysis**: Explore statistical significance and confidence intervals\n",
        "3. **Custom Metrics**: Create domain-specific performance measures\n",
        "4. **Interactive Dashboards**: Build web-based leaderboard interfaces\n",
        "\n",
        "### Files Generated\n",
        "\n",
        "- `outputs/performance_leaderboard.csv`: Complete leaderboard data\n",
        "- `outputs/performance_data.csv`: Raw performance data\n",
        "- `outputs/performance_data.json`: JSON format data\n",
        "- `outputs/leaderboard_table.tex`: LaTeX table for publications\n",
        "- `outputs/leaderboard_visualization.png`: Comprehensive visualization\n",
        "\n",
        "### References\n",
        "\n",
        "1. Taqqu, M. S., Teverovsky, V., & Willinger, W. (1995). Estimators for long-range dependence: an empirical study. Fractals, 3(04), 785-798.\n",
        "2. Beran, J. (1994). Statistics for long-memory processes. CRC press.\n",
        "3. Abry, P., & Veitch, D. (1998). Wavelet analysis of long-range-dependent traffic. IEEE Transactions on information theory, 44(1), 2-15.\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've completed the comprehensive LRDBenchmark demonstration series. You now have a complete understanding of:\n",
        "- Data generation and visualization\n",
        "- Estimation and statistical validation\n",
        "- Custom model and estimator development\n",
        "- Comprehensive benchmarking\n",
        "- Leaderboard generation and analysis\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
