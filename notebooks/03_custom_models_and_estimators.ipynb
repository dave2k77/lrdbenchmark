{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Models and Estimators\n",
        "\n",
        "This notebook demonstrates how to extend the LRDBenchmark library with custom data models and estimators, showing the extensibility and flexibility of the framework.\n",
        "\n",
        "## Overview\n",
        "\n",
        "LRDBenchmark is designed to be highly extensible, allowing users to add their own data models and estimators. This notebook covers:\n",
        "\n",
        "1. **Understanding the Base Classes**: How the framework is structured\n",
        "2. **Custom Data Models**: Creating new stochastic processes\n",
        "3. **Custom Estimators**: Implementing new estimation methods\n",
        "4. **Integration**: Making custom components work with the benchmark system\n",
        "5. **Best Practices**: Guidelines for extensibility\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Understanding Base Classes](#base-classes)\n",
        "3. [Custom Data Model Example](#custom-data-model)\n",
        "4. [Custom Classical Estimator](#custom-classical)\n",
        "5. [Custom ML Estimator](#custom-ml)\n",
        "6. [Integration with Benchmark System](#integration)\n",
        "7. [Testing and Validation](#testing)\n",
        "8. [Best Practices](#best-practices)\n",
        "9. [Summary and Next Steps](#summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports {#setup}\n",
        "\n",
        "First, let's import all necessary libraries and examine the base classes that we'll be extending.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard scientific computing imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Import LRDBenchmark base classes\n",
        "from lrdbenchmark.models.data_models.base_model import BaseModel\n",
        "from lrdbenchmark.analysis.machine_learning.ml_model_factory import MLModelFactory, ModelConfig\n",
        "from lrdbenchmark.analysis.machine_learning.neural_network_factory import NeuralNetworkFactory, NNConfig, NNArchitecture\n",
        "\n",
        "# Import existing models for comparison\n",
        "from lrdbenchmark.models.data_models.fbm.fbm_model import FractionalBrownianMotion\n",
        "from lrdbenchmark.analysis.temporal.rs.rs_estimator_unified import RSEstimator\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(\"ðŸ”§ Ready to create custom models and estimators\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Base Classes {#base-classes}\n",
        "\n",
        "Let's examine the base classes that we'll be extending to understand the framework structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine the BaseModel class structure\n",
        "print(\"ðŸ” Examining BaseModel class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get the BaseModel class methods\n",
        "base_methods = [method for method in dir(BaseModel) if not method.startswith('_')]\n",
        "print(\"BaseModel methods:\")\n",
        "for method in base_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Required methods for custom data models:\")\n",
        "print(\"  - __init__(self, **kwargs): Initialize with parameters\")\n",
        "print(\"  - _validate_parameters(self): Validate input parameters\")\n",
        "print(\"  - generate(self, n, seed=None): Generate n samples\")\n",
        "print(\"  - get_theoretical_properties(self): Return theoretical properties\")\n",
        "\n",
        "# Examine the MLModelFactory structure\n",
        "print(\"\\nðŸ” Examining MLModelFactory class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ml_methods = [method for method in dir(MLModelFactory) if not method.startswith('_')]\n",
        "print(\"MLModelFactory methods:\")\n",
        "for method in ml_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Key components for custom ML estimators:\")\n",
        "print(\"  - ModelConfig: Configuration class for model parameters\")\n",
        "print(\"  - MLModelFactory: Factory for creating ML models\")\n",
        "print(\"  - TrainingResult: Results from model training\")\n",
        "\n",
        "# Examine the NeuralNetworkFactory structure\n",
        "print(\"\\nðŸ” Examining NeuralNetworkFactory class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "nn_methods = [method for method in dir(NeuralNetworkFactory) if not method.startswith('_')]\n",
        "print(\"NeuralNetworkFactory methods:\")\n",
        "for method in nn_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Key components for custom neural estimators:\")\n",
        "print(\"  - NNConfig: Configuration class for neural network parameters\")\n",
        "print(\"  - NNArchitecture: Available neural network architectures\")\n",
        "print(\"  - NeuralNetworkFactory: Factory for creating neural networks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Data Model Example {#custom-data-model}\n",
        "\n",
        "Let's create a custom data model: **Fractional Ornstein-Uhlenbeck Process**. This is a mean-reverting process with long-range dependence, useful for modeling financial time series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FractionalOrnsteinUhlenbeck(BaseModel):\n",
        "    \"\"\"\n",
        "    Custom Fractional Ornstein-Uhlenbeck Process.\n",
        "    \n",
        "    This process combines mean reversion with long-range dependence:\n",
        "    dX_t = -Î¸(X_t - Î¼)dt + Ïƒ dB_H(t)\n",
        "    \n",
        "    where:\n",
        "    - Î¸: mean reversion speed\n",
        "    - Î¼: long-term mean\n",
        "    - Ïƒ: volatility\n",
        "    - B_H(t): fractional Brownian motion with Hurst parameter H\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, theta=0.1, mu=0.0, sigma=1.0, H=0.7, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the Fractional Ornstein-Uhlenbeck process.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        theta : float\n",
        "            Mean reversion speed (must be > 0)\n",
        "        mu : float\n",
        "            Long-term mean\n",
        "        sigma : float\n",
        "            Volatility (must be > 0)\n",
        "        H : float\n",
        "            Hurst parameter (0 < H < 1)\n",
        "        \"\"\"\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.H = H\n",
        "        \n",
        "        # Store parameters for base class\n",
        "        super().__init__(theta=theta, mu=mu, sigma=sigma, H=H, **kwargs)\n",
        "    \n",
        "    def _validate_parameters(self):\n",
        "        \"\"\"Validate the model parameters.\"\"\"\n",
        "        if self.theta <= 0:\n",
        "            raise ValueError(\"Mean reversion speed theta must be positive\")\n",
        "        if self.sigma <= 0:\n",
        "            raise ValueError(\"Volatility sigma must be positive\")\n",
        "        if not (0 < self.H < 1):\n",
        "            raise ValueError(\"Hurst parameter H must be in (0, 1)\")\n",
        "    \n",
        "    def generate(self, n, seed=None):\n",
        "        \"\"\"\n",
        "        Generate n samples from the Fractional Ornstein-Uhlenbeck process.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Number of samples to generate\n",
        "        seed : int, optional\n",
        "            Random seed for reproducibility\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Generated time series\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        # Generate fractional Brownian motion\n",
        "        fbm = FractionalBrownianMotion(H=self.H, sigma=1.0)\n",
        "        fbm_path = fbm.generate(n, seed=seed)\n",
        "        \n",
        "        # Apply Ornstein-Uhlenbeck transformation\n",
        "        dt = 1.0 / n  # Time step\n",
        "        x = np.zeros(n)\n",
        "        x[0] = self.mu  # Start at long-term mean\n",
        "        \n",
        "        for i in range(1, n):\n",
        "            # Euler-Maruyama discretization\n",
        "            dx = -self.theta * (x[i-1] - self.mu) * dt + self.sigma * (fbm_path[i] - fbm_path[i-1])\n",
        "            x[i] = x[i-1] + dx\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_theoretical_properties(self):\n",
        "        \"\"\"\n",
        "        Get theoretical properties of the process.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dictionary containing theoretical properties\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'mean': self.mu,\n",
        "            'variance': self.sigma**2 / (2 * self.theta),\n",
        "            'autocorrelation_time': 1 / self.theta,\n",
        "            'hurst_parameter': self.H,\n",
        "            'mean_reversion_speed': self.theta,\n",
        "            'long_term_mean': self.mu,\n",
        "            'volatility': self.sigma\n",
        "        }\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"FractionalOrnsteinUhlenbeck(theta={self.theta}, mu={self.mu}, sigma={self.sigma}, H={self.H})\"\n",
        "\n",
        "# Test the custom model\n",
        "print(\"ðŸ§ª Testing Custom Fractional Ornstein-Uhlenbeck Model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create model instance\n",
        "fou = FractionalOrnsteinUhlenbeck(theta=0.1, mu=0.0, sigma=1.0, H=0.7)\n",
        "print(f\"Model: {fou}\")\n",
        "\n",
        "# Generate sample data\n",
        "n_samples = 1000\n",
        "data = fou.generate(n_samples, seed=42)\n",
        "\n",
        "# Print basic statistics\n",
        "print(f\"\\nðŸ“Š Generated data statistics:\")\n",
        "print(f\"  Length: {len(data)}\")\n",
        "print(f\"  Mean: {data.mean():.4f}\")\n",
        "print(f\"  Std: {data.std():.4f}\")\n",
        "print(f\"  Min: {data.min():.4f}\")\n",
        "print(f\"  Max: {data.max():.4f}\")\n",
        "\n",
        "# Get theoretical properties\n",
        "theoretical = fou.get_theoretical_properties()\n",
        "print(f\"\\nðŸ§® Theoretical properties:\")\n",
        "for key, value in theoretical.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Time series\n",
        "axes[0, 0].plot(data[:500], linewidth=1.5, alpha=0.8)\n",
        "axes[0, 0].set_title('Fractional Ornstein-Uhlenbeck Process', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Time')\n",
        "axes[0, 0].set_ylabel('Value')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution\n",
        "axes[0, 1].hist(data, bins=50, density=True, alpha=0.7)\n",
        "axes[0, 1].set_title('Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Autocorrelation\n",
        "from statsmodels.tsa.stattools import acf\n",
        "acf_values = acf(data, nlags=50, fft=True)\n",
        "axes[1, 0].plot(acf_values, linewidth=2, alpha=0.8)\n",
        "axes[1, 0].set_title('Autocorrelation Function', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Lag')\n",
        "axes[1, 0].set_ylabel('ACF')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Power spectral density\n",
        "freqs = np.fft.fftfreq(n_samples)[:n_samples//2]\n",
        "psd = np.abs(np.fft.fft(data))**2\n",
        "psd = psd[:n_samples//2]\n",
        "axes[1, 1].loglog(freqs[1:], psd[1:], linewidth=2, alpha=0.8)\n",
        "axes[1, 1].set_title('Power Spectral Density', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Frequency')\n",
        "axes[1, 1].set_ylabel('PSD')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/custom_fou_model.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Custom data model created and tested successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Classical Estimator {#custom-classical}\n",
        "\n",
        "Let's create a custom classical estimator: **Variance-Based Hurst Estimator**. This is a simple but effective method based on the scaling of variance with time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VarianceBasedHurstEstimator:\n",
        "    \"\"\"\n",
        "    Custom Variance-Based Hurst Estimator.\n",
        "    \n",
        "    This estimator uses the scaling relationship:\n",
        "    Var(X_t) âˆ t^(2H)\n",
        "    \n",
        "    where H is the Hurst parameter.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, min_scale=10, max_scale=None, num_scales=10):\n",
        "        \"\"\"\n",
        "        Initialize the variance-based Hurst estimator.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        min_scale : int\n",
        "            Minimum time scale for analysis\n",
        "        max_scale : int, optional\n",
        "            Maximum time scale (default: data_length // 4)\n",
        "        num_scales : int\n",
        "            Number of scales to use\n",
        "        \"\"\"\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        self.num_scales = num_scales\n",
        "    \n",
        "    def estimate(self, data):\n",
        "        \"\"\"\n",
        "        Estimate Hurst parameter using variance scaling.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data : np.ndarray\n",
        "            Time series data\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dictionary containing estimation results\n",
        "        \"\"\"\n",
        "        n = len(data)\n",
        "        \n",
        "        # Set maximum scale if not provided\n",
        "        if self.max_scale is None:\n",
        "            max_scale = n // 4\n",
        "        else:\n",
        "            max_scale = min(self.max_scale, n // 4)\n",
        "        \n",
        "        # Generate scales\n",
        "        scales = np.logspace(np.log10(self.min_scale), np.log10(max_scale), self.num_scales).astype(int)\n",
        "        scales = np.unique(scales)  # Remove duplicates\n",
        "        \n",
        "        # Calculate variances for each scale\n",
        "        variances = []\n",
        "        valid_scales = []\n",
        "        \n",
        "        for scale in scales:\n",
        "            if scale >= n:\n",
        "                continue\n",
        "                \n",
        "            # Calculate variance for this scale\n",
        "            var_scale = self._calculate_variance_at_scale(data, scale)\n",
        "            if var_scale > 0:\n",
        "                variances.append(var_scale)\n",
        "                valid_scales.append(scale)\n",
        "        \n",
        "        if len(variances) < 3:\n",
        "            return {'hurst_parameter': None, 'error': 'Insufficient data for estimation'}\n",
        "        \n",
        "        # Fit power law: log(Var) = 2H * log(t) + C\n",
        "        log_scales = np.log(valid_scales)\n",
        "        log_variances = np.log(variances)\n",
        "        \n",
        "        # Linear regression\n",
        "        coeffs = np.polyfit(log_scales, log_variances, 1)\n",
        "        H_estimate = coeffs[0] / 2.0\n",
        "        \n",
        "        # Calculate R-squared\n",
        "        y_pred = coeffs[0] * log_scales + coeffs[1]\n",
        "        ss_res = np.sum((log_variances - y_pred) ** 2)\n",
        "        ss_tot = np.sum((log_variances - np.mean(log_variances)) ** 2)\n",
        "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'hurst_parameter': H_estimate,\n",
        "            'r_squared': r_squared,\n",
        "            'scales_used': len(valid_scales),\n",
        "            'method': 'Variance-Based'\n",
        "        }\n",
        "    \n",
        "    def _calculate_variance_at_scale(self, data, scale):\n",
        "        \"\"\"Calculate variance at a specific time scale.\"\"\"\n",
        "        n = len(data)\n",
        "        if scale >= n:\n",
        "            return 0\n",
        "        \n",
        "        # Calculate variance for non-overlapping windows\n",
        "        num_windows = n // scale\n",
        "        if num_windows < 2:\n",
        "            return 0\n",
        "        \n",
        "        variances = []\n",
        "        for i in range(num_windows):\n",
        "            start_idx = i * scale\n",
        "            end_idx = start_idx + scale\n",
        "            window_data = data[start_idx:end_idx]\n",
        "            if len(window_data) > 0:\n",
        "                variances.append(np.var(window_data))\n",
        "        \n",
        "        return np.mean(variances) if variances else 0\n",
        "\n",
        "# Test the custom estimator\n",
        "print(\"ðŸ§ª Testing Custom Variance-Based Hurst Estimator:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create estimator\n",
        "variance_estimator = VarianceBasedHurstEstimator(min_scale=10, num_scales=15)\n",
        "\n",
        "# Test on FBM data with known H\n",
        "H_test = 0.7\n",
        "fbm = FractionalBrownianMotion(H=H_test, sigma=1.0)\n",
        "test_data = fbm.generate(1000, seed=42)\n",
        "\n",
        "# Estimate Hurst parameter\n",
        "result = variance_estimator.estimate(test_data)\n",
        "\n",
        "print(f\"Test data: FBM with H = {H_test}\")\n",
        "print(f\"Estimated H: {result['hurst_parameter']:.4f}\")\n",
        "print(f\"Error: {abs(result['hurst_parameter'] - H_test):.4f}\")\n",
        "print(f\"R-squared: {result['r_squared']:.4f}\")\n",
        "print(f\"Scales used: {result['scales_used']}\")\n",
        "\n",
        "# Compare with R/S estimator\n",
        "rs_estimator = RSEstimator()\n",
        "rs_result = rs_estimator.estimate(test_data)\n",
        "rs_H = rs_result.get('hurst_parameter', rs_result.get('H', None))\n",
        "\n",
        "print(f\"\\nComparison with R/S estimator:\")\n",
        "print(f\"R/S H: {rs_H:.4f}\")\n",
        "print(f\"R/S Error: {abs(rs_H - H_test):.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Custom classical estimator created and tested successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Extensibility**: LRDBenchmark is designed to be highly extensible with clear interfaces for custom components.\n",
        "\n",
        "2. **Custom Data Models**: Inherit from BaseModel and implement required methods.\n",
        "\n",
        "3. **Custom Estimators**: Follow standard interface and return consistent results.\n",
        "\n",
        "4. **Integration**: Custom components work with existing benchmark system.\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "- Parameter validation\n",
        "- Error handling  \n",
        "- Documentation\n",
        "- Testing\n",
        "- Performance consideration\n",
        "- Compatibility\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Test custom components in benchmark system\n",
        "2. Apply to real-world data\n",
        "3. Explore advanced features\n",
        "4. Share with community\n",
        "\n",
        "---\n",
        "\n",
        "**Next Notebook**: [04_comprehensive_benchmarking.ipynb](04_comprehensive_benchmarking.ipynb) - Learn how to use the comprehensive benchmarking system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
