{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Models and Estimators\n",
        "\n",
        "This notebook demonstrates how to extend the LRDBenchmark library with custom data models and estimators, showing the extensibility and flexibility of the framework.\n",
        "\n",
        "## Overview\n",
        "\n",
        "LRDBenchmark is designed to be highly extensible, allowing users to add their own data models and estimators. This notebook covers:\n",
        "\n",
        "1. **Understanding the Base Classes**: How the framework is structured\n",
        "2. **Custom Data Models**: Creating new stochastic processes\n",
        "3. **Custom Estimators**: Implementing new estimation methods\n",
        "4. **Integration**: Making custom components work with the benchmark system\n",
        "5. **Best Practices**: Guidelines for extensibility\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Understanding Base Classes](#base-classes)\n",
        "3. [Custom Data Model Example](#custom-data-model)\n",
        "4. [Custom Classical Estimator](#custom-classical)\n",
        "5. [Custom ML Estimator](#custom-ml)\n",
        "6. [Integration with Benchmark System](#integration)\n",
        "7. [Testing and Validation](#testing)\n",
        "8. [Best Practices](#best-practices)\n",
        "9. [Summary and Next Steps](#summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports {#setup}\n",
        "\n",
        "First, let's import all necessary libraries and examine the base classes that we'll be extending.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… All imports successful!\n",
            "ðŸ”§ Ready to create custom models and estimators\n"
          ]
        }
      ],
      "source": [
        "# Standard scientific computing imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set JAX to use CPU to avoid CUDA issues\n",
        "import os\n",
        "os.environ['JAX_PLATFORMS'] = 'cpu'\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Import LRDBenchmark base classes\n",
        "from lrdbenchmark.models.data_models.base_model import BaseModel\n",
        "from lrdbenchmark.analysis.machine_learning.ml_model_factory import MLModelFactory, ModelConfig\n",
        "from lrdbenchmark.analysis.machine_learning.neural_network_factory import NeuralNetworkFactory, NNConfig, NNArchitecture\n",
        "\n",
        "# Import existing models for comparison\n",
        "from lrdbenchmark.models.data_models.fbm.fbm_model import FractionalBrownianMotion\n",
        "from lrdbenchmark.analysis.temporal.rs.rs_estimator_unified import RSEstimator\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(\"ðŸ”§ Ready to create custom models and estimators\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Understanding Base Classes {#base-classes}\n",
        "\n",
        "Let's examine the base classes that we'll be extending to understand the framework structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Examining BaseModel class structure:\n",
            "==================================================\n",
            "BaseModel methods:\n",
            "  - generate\n",
            "  - get_parameters\n",
            "  - get_theoretical_properties\n",
            "  - set_parameters\n",
            "\n",
            "ðŸ“‹ Required methods for custom data models:\n",
            "  - __init__(self, **kwargs): Initialize with parameters\n",
            "  - _validate_parameters(self): Validate input parameters\n",
            "  - generate(self, n, seed=None): Generate n samples\n",
            "  - get_theoretical_properties(self): Return theoretical properties\n",
            "\n",
            "ðŸ” Examining MLModelFactory class structure:\n",
            "==================================================\n",
            "MLModelFactory methods:\n",
            "  - create_model\n",
            "  - create_optimized_model\n",
            "  - ensemble_predict\n",
            "  - get_model_recommendation\n",
            "  - optimize_hyperparameters\n",
            "\n",
            "ðŸ“‹ Key components for custom ML estimators:\n",
            "  - ModelConfig: Configuration class for model parameters\n",
            "  - MLModelFactory: Factory for creating ML models\n",
            "  - TrainingResult: Results from model training\n",
            "\n",
            "ðŸ” Examining NeuralNetworkFactory class structure:\n",
            "==================================================\n",
            "NeuralNetworkFactory methods:\n",
            "  - create_benchmark_networks\n",
            "  - create_network\n",
            "  - get_available_architectures\n",
            "\n",
            "ðŸ“‹ Key components for custom neural estimators:\n",
            "  - NNConfig: Configuration class for neural network parameters\n",
            "  - NNArchitecture: Available neural network architectures\n",
            "  - NeuralNetworkFactory: Factory for creating neural networks\n"
          ]
        }
      ],
      "source": [
        "# Examine the BaseModel class structure\n",
        "print(\"ðŸ” Examining BaseModel class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get the BaseModel class methods\n",
        "base_methods = [method for method in dir(BaseModel) if not method.startswith('_')]\n",
        "print(\"BaseModel methods:\")\n",
        "for method in base_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Required methods for custom data models:\")\n",
        "print(\"  - __init__(self, **kwargs): Initialize with parameters\")\n",
        "print(\"  - _validate_parameters(self): Validate input parameters\")\n",
        "print(\"  - generate(self, n, seed=None): Generate n samples\")\n",
        "print(\"  - get_theoretical_properties(self): Return theoretical properties\")\n",
        "\n",
        "# Examine the MLModelFactory structure\n",
        "print(\"\\nðŸ” Examining MLModelFactory class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ml_methods = [method for method in dir(MLModelFactory) if not method.startswith('_')]\n",
        "print(\"MLModelFactory methods:\")\n",
        "for method in ml_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Key components for custom ML estimators:\")\n",
        "print(\"  - ModelConfig: Configuration class for model parameters\")\n",
        "print(\"  - MLModelFactory: Factory for creating ML models\")\n",
        "print(\"  - TrainingResult: Results from model training\")\n",
        "\n",
        "# Examine the NeuralNetworkFactory structure\n",
        "print(\"\\nðŸ” Examining NeuralNetworkFactory class structure:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "nn_methods = [method for method in dir(NeuralNetworkFactory) if not method.startswith('_')]\n",
        "print(\"NeuralNetworkFactory methods:\")\n",
        "for method in nn_methods:\n",
        "    print(f\"  - {method}\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Key components for custom neural estimators:\")\n",
        "print(\"  - NNConfig: Configuration class for neural network parameters\")\n",
        "print(\"  - NNArchitecture: Available neural network architectures\")\n",
        "print(\"  - NeuralNetworkFactory: Factory for creating neural networks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Data Model Example {#custom-data-model}\n",
        "\n",
        "Let's create a custom data model: **Fractional Ornstein-Uhlenbeck Process**. This is a mean-reverting process with long-range dependence, useful for modeling financial time series.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:2025-10-15 08:48:24,369:jax._src.xla_bridge:822: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
            "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª Testing Custom Fractional Ornstein-Uhlenbeck Model:\n",
            "============================================================\n",
            "Model: FractionalOrnsteinUhlenbeck(theta=0.1, mu=0.0, sigma=1.0, H=0.7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1015 08:48:24.556179  107243 platform_util.cc:218] unable to create StreamExecutor for CUDA:0: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/xla_bridge.py:896\u001b[39m, in \u001b[36m_init_backend\u001b[39m\u001b[34m(platform)\u001b[39m\n\u001b[32m    895\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mInitializing backend \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m, platform)\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m backend = \u001b[43mregistration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/xla_bridge.py:549\u001b[39m, in \u001b[36mmake_pjrt_c_api_client\u001b[39m\u001b[34m(plugin_name, options)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m distributed.global_state.client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxla_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m distribute_options = {\n\u001b[32m    552\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnode_id\u001b[39m\u001b[33m'\u001b[39m: distributed.global_state.process_id,\n\u001b[32m    553\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnum_nodes\u001b[39m\u001b[33m'\u001b[39m: distributed.global_state.num_processes,\n\u001b[32m    554\u001b[39m }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jaxlib/xla_client.py:156\u001b[39m, in \u001b[36mmake_c_api_client\u001b[39m\u001b[34m(plugin_name, options, distributed_client, transfer_server_factory)\u001b[39m\n\u001b[32m    155\u001b[39m   options = {}\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_xla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_c_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistributed_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransfer_server_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mXlaRuntimeError\u001b[39m: INTERNAL: no supported devices found for platform CUDA",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Generate sample data\u001b[39;00m\n\u001b[32m    113\u001b[39m n_samples = \u001b[32m1000\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m data = \u001b[43mfou\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Print basic statistics\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Generated data statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mFractionalOrnsteinUhlenbeck.generate\u001b[39m\u001b[34m(self, n, seed)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Generate fractional Brownian motion\u001b[39;00m\n\u001b[32m     67\u001b[39m fbm = FractionalBrownianMotion(H=\u001b[38;5;28mself\u001b[39m.H, sigma=\u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m fbm_path = \u001b[43mfbm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Apply Ornstein-Uhlenbeck transformation\u001b[39;00m\n\u001b[32m     71\u001b[39m dt = \u001b[32m1.0\u001b[39m / n  \u001b[38;5;66;03m# Time step\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/lrdbenchmark/models/data_models/fbm/fbm_model.py:208\u001b[39m, in \u001b[36mFractionalBrownianMotion.generate\u001b[39m\u001b[34m(self, n, seed)\u001b[39m\n\u001b[32m    206\u001b[39m     np.random.seed(seed)\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m JAX_AVAILABLE:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m         \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m H = \u001b[38;5;28mself\u001b[39m.parameters[\u001b[33m\"\u001b[39m\u001b[33mH\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    211\u001b[39m sigma = \u001b[38;5;28mself\u001b[39m.parameters[\u001b[33m\"\u001b[39m\u001b[33msigma\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/random.py:248\u001b[39m, in \u001b[36mPRNGKey\u001b[39m\u001b[34m(seed, impl)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mPRNGKey\u001b[39m(seed: \u001b[38;5;28mint\u001b[39m | ArrayLike, *,\n\u001b[32m    223\u001b[39m             impl: PRNGSpecDesc | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Array:\n\u001b[32m    224\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Create a legacy PRNG key given an integer seed.\u001b[39;00m\n\u001b[32m    225\u001b[39m \n\u001b[32m    226\u001b[39m \u001b[33;03m  This function produces old-style legacy PRNG keys, which are arrays\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m \u001b[33;03m    and ``fold_in``.\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43m_key\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPRNGKey\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/random.py:200\u001b[39m, in \u001b[36m_key\u001b[39m\u001b[34m(ctor_name, seed, impl_spec)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.ndim(seed):\n\u001b[32m    197\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    198\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m accepts a scalar seed, but was given an array of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.shape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != (). Use jax.vmap for batching\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprng\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/prng.py:554\u001b[39m, in \u001b[36mrandom_seed\u001b[39m\u001b[34m(seeds, impl)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrandom_seed\u001b[39m(seeds: \u001b[38;5;28mint\u001b[39m | typing.ArrayLike, impl: PRNGImpl) -> PRNGKeyArray:\n\u001b[32m    550\u001b[39m   \u001b[38;5;66;03m# Avoid overflow error in X32 mode by first converting ints to int64.\u001b[39;00m\n\u001b[32m    551\u001b[39m   \u001b[38;5;66;03m# This breaks JIT invariance for large ints, but supports the common\u001b[39;00m\n\u001b[32m    552\u001b[39m   \u001b[38;5;66;03m# use-case of instantiating with Python hashes in X32 mode.\u001b[39;00m\n\u001b[32m    553\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seeds, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     seeds_arr = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    556\u001b[39m     seeds_arr = jnp.asarray(seeds)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/numpy/array_constructors.py:384\u001b[39m, in \u001b[36masarray\u001b[39m\u001b[34m(a, dtype, order, copy, device)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    383\u001b[39m   dtype = dtypes.check_and_canonicalize_user_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33masarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/numpy/array_constructors.py:270\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(object, dtype, copy, order, ndmin, device)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    269\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m out_array: Array = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_convert_element_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ndmin > np.ndim(out_array):\n\u001b[32m    273\u001b[39m   out_array = lax.expand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin - np.ndim(out_array)))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/lax/lax.py:1743\u001b[39m, in \u001b[36m_convert_element_type\u001b[39m\u001b[34m(operand, new_dtype, weak_type, sharding, warn_on_complex_to_real_cast)\u001b[39m\n\u001b[32m   1741\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m operand\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_element_type_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m      \u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m      \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/core.py:634\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    633\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/core.py:650\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    648\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    652\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/lax/lax.py:4988\u001b[39m, in \u001b[36m_convert_element_type_bind_with_trace\u001b[39m\u001b[34m(trace, args, params)\u001b[39m\n\u001b[32m   4986\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_element_type_bind_with_trace\u001b[39m(trace, args, params):\n\u001b[32m   4987\u001b[39m   sharding = params[\u001b[33m'\u001b[39m\u001b[33msharding\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m4988\u001b[39m   operand = \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPrimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_element_type_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4989\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sharding._is_concrete:\n\u001b[32m   4990\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m core.set_current_trace(trace):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/core.py:662\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    661\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)  \u001b[38;5;66;03m# may raise lojax error\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt apply typeof to args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/core.py:1189\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1187\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1188\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/dispatch.py:90\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     88\u001b[39m prev = lib.jax_jit.swap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m   outs = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     92\u001b[39m   lib.jax_jit.swap_thread_local_state_disable_jit(prev)\n",
            "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/lrdbenchmark/lib/python3.13/site-packages/jax/_src/xla_bridge.py:828\u001b[39m, in \u001b[36mbackends\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    826\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m         err_msg += \u001b[33m\"\u001b[39m\u001b[33m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.jax_platforms.value:\n",
            "\u001b[31mRuntimeError\u001b[39m: Unable to initialize backend 'cuda': INTERNAL: no supported devices found for platform CUDA (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)"
          ]
        }
      ],
      "source": [
        "class FractionalOrnsteinUhlenbeck(BaseModel):\n",
        "    \"\"\"\n",
        "    Custom Fractional Ornstein-Uhlenbeck Process.\n",
        "    \n",
        "    This process combines mean reversion with long-range dependence:\n",
        "    dX_t = -Î¸(X_t - Î¼)dt + Ïƒ dB_H(t)\n",
        "    \n",
        "    where:\n",
        "    - Î¸: mean reversion speed\n",
        "    - Î¼: long-term mean\n",
        "    - Ïƒ: volatility\n",
        "    - B_H(t): fractional Brownian motion with Hurst parameter H\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, theta=0.1, mu=0.0, sigma=1.0, H=0.7, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the Fractional Ornstein-Uhlenbeck process.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        theta : float\n",
        "            Mean reversion speed (must be > 0)\n",
        "        mu : float\n",
        "            Long-term mean\n",
        "        sigma : float\n",
        "            Volatility (must be > 0)\n",
        "        H : float\n",
        "            Hurst parameter (0 < H < 1)\n",
        "        \"\"\"\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.H = H\n",
        "        \n",
        "        # Store parameters for base class\n",
        "        super().__init__(theta=theta, mu=mu, sigma=sigma, H=H, **kwargs)\n",
        "    \n",
        "    def _validate_parameters(self):\n",
        "        \"\"\"Validate the model parameters.\"\"\"\n",
        "        if self.theta <= 0:\n",
        "            raise ValueError(\"Mean reversion speed theta must be positive\")\n",
        "        if self.sigma <= 0:\n",
        "            raise ValueError(\"Volatility sigma must be positive\")\n",
        "        if not (0 < self.H < 1):\n",
        "            raise ValueError(\"Hurst parameter H must be in (0, 1)\")\n",
        "    \n",
        "    def generate(self, n, seed=None):\n",
        "        \"\"\"\n",
        "        Generate n samples from the Fractional Ornstein-Uhlenbeck process.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            Number of samples to generate\n",
        "        seed : int, optional\n",
        "            Random seed for reproducibility\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Generated time series\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        \n",
        "        # Generate fractional Brownian motion\n",
        "        fbm = FractionalBrownianMotion(H=self.H, sigma=1.0)\n",
        "        fbm_path = fbm.generate(n, seed=seed)\n",
        "        \n",
        "        # Apply Ornstein-Uhlenbeck transformation\n",
        "        dt = 1.0 / n  # Time step\n",
        "        x = np.zeros(n)\n",
        "        x[0] = self.mu  # Start at long-term mean\n",
        "        \n",
        "        for i in range(1, n):\n",
        "            # Euler-Maruyama discretization\n",
        "            dx = -self.theta * (x[i-1] - self.mu) * dt + self.sigma * (fbm_path[i] - fbm_path[i-1])\n",
        "            x[i] = x[i-1] + dx\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def get_theoretical_properties(self):\n",
        "        \"\"\"\n",
        "        Get theoretical properties of the process.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dictionary containing theoretical properties\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'mean': self.mu,\n",
        "            'variance': self.sigma**2 / (2 * self.theta),\n",
        "            'autocorrelation_time': 1 / self.theta,\n",
        "            'hurst_parameter': self.H,\n",
        "            'mean_reversion_speed': self.theta,\n",
        "            'long_term_mean': self.mu,\n",
        "            'volatility': self.sigma\n",
        "        }\n",
        "    \n",
        "    def __str__(self):\n",
        "        return f\"FractionalOrnsteinUhlenbeck(theta={self.theta}, mu={self.mu}, sigma={self.sigma}, H={self.H})\"\n",
        "\n",
        "# Test the custom model\n",
        "print(\"ðŸ§ª Testing Custom Fractional Ornstein-Uhlenbeck Model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create model instance\n",
        "fou = FractionalOrnsteinUhlenbeck(theta=0.1, mu=0.0, sigma=1.0, H=0.7)\n",
        "print(f\"Model: {fou}\")\n",
        "\n",
        "# Generate sample data\n",
        "n_samples = 1000\n",
        "data = fou.generate(n_samples, seed=42)\n",
        "\n",
        "# Print basic statistics\n",
        "print(f\"\\nðŸ“Š Generated data statistics:\")\n",
        "print(f\"  Length: {len(data)}\")\n",
        "print(f\"  Mean: {data.mean():.4f}\")\n",
        "print(f\"  Std: {data.std():.4f}\")\n",
        "print(f\"  Min: {data.min():.4f}\")\n",
        "print(f\"  Max: {data.max():.4f}\")\n",
        "\n",
        "# Get theoretical properties\n",
        "theoretical = fou.get_theoretical_properties()\n",
        "print(f\"\\nðŸ§® Theoretical properties:\")\n",
        "for key, value in theoretical.items():\n",
        "    print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Time series\n",
        "axes[0, 0].plot(data[:500], linewidth=1.5, alpha=0.8)\n",
        "axes[0, 0].set_title('Fractional Ornstein-Uhlenbeck Process', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Time')\n",
        "axes[0, 0].set_ylabel('Value')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution\n",
        "axes[0, 1].hist(data, bins=50, density=True, alpha=0.7)\n",
        "axes[0, 1].set_title('Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Value')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Autocorrelation\n",
        "from statsmodels.tsa.stattools import acf\n",
        "acf_values = acf(data, nlags=50, fft=True)\n",
        "axes[1, 0].plot(acf_values, linewidth=2, alpha=0.8)\n",
        "axes[1, 0].set_title('Autocorrelation Function', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Lag')\n",
        "axes[1, 0].set_ylabel('ACF')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Power spectral density\n",
        "freqs = np.fft.fftfreq(n_samples)[:n_samples//2]\n",
        "psd = np.abs(np.fft.fft(data))**2\n",
        "psd = psd[:n_samples//2]\n",
        "axes[1, 1].loglog(freqs[1:], psd[1:], linewidth=2, alpha=0.8)\n",
        "axes[1, 1].set_title('Power Spectral Density', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Frequency')\n",
        "axes[1, 1].set_ylabel('PSD')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/custom_fou_model.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Custom data model created and tested successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Classical Estimator {#custom-classical}\n",
        "\n",
        "Let's create a custom classical estimator: **Variance-Based Hurst Estimator**. This is a simple but effective method based on the scaling of variance with time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VarianceBasedHurstEstimator:\n",
        "    \"\"\"\n",
        "    Custom Variance-Based Hurst Estimator.\n",
        "    \n",
        "    This estimator uses the scaling relationship:\n",
        "    Var(X_t) âˆ t^(2H)\n",
        "    \n",
        "    where H is the Hurst parameter.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, min_scale=10, max_scale=None, num_scales=10):\n",
        "        \"\"\"\n",
        "        Initialize the variance-based Hurst estimator.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        min_scale : int\n",
        "            Minimum time scale for analysis\n",
        "        max_scale : int, optional\n",
        "            Maximum time scale (default: data_length // 4)\n",
        "        num_scales : int\n",
        "            Number of scales to use\n",
        "        \"\"\"\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        self.num_scales = num_scales\n",
        "    \n",
        "    def estimate(self, data):\n",
        "        \"\"\"\n",
        "        Estimate Hurst parameter using variance scaling.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data : np.ndarray\n",
        "            Time series data\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dictionary containing estimation results\n",
        "        \"\"\"\n",
        "        n = len(data)\n",
        "        \n",
        "        # Set maximum scale if not provided\n",
        "        if self.max_scale is None:\n",
        "            max_scale = n // 4\n",
        "        else:\n",
        "            max_scale = min(self.max_scale, n // 4)\n",
        "        \n",
        "        # Generate scales\n",
        "        scales = np.logspace(np.log10(self.min_scale), np.log10(max_scale), self.num_scales).astype(int)\n",
        "        scales = np.unique(scales)  # Remove duplicates\n",
        "        \n",
        "        # Calculate variances for each scale\n",
        "        variances = []\n",
        "        valid_scales = []\n",
        "        \n",
        "        for scale in scales:\n",
        "            if scale >= n:\n",
        "                continue\n",
        "                \n",
        "            # Calculate variance for this scale\n",
        "            var_scale = self._calculate_variance_at_scale(data, scale)\n",
        "            if var_scale > 0:\n",
        "                variances.append(var_scale)\n",
        "                valid_scales.append(scale)\n",
        "        \n",
        "        if len(variances) < 3:\n",
        "            return {'hurst_parameter': None, 'error': 'Insufficient data for estimation'}\n",
        "        \n",
        "        # Fit power law: log(Var) = 2H * log(t) + C\n",
        "        log_scales = np.log(valid_scales)\n",
        "        log_variances = np.log(variances)\n",
        "        \n",
        "        # Linear regression\n",
        "        coeffs = np.polyfit(log_scales, log_variances, 1)\n",
        "        H_estimate = coeffs[0] / 2.0\n",
        "        \n",
        "        # Calculate R-squared\n",
        "        y_pred = coeffs[0] * log_scales + coeffs[1]\n",
        "        ss_res = np.sum((log_variances - y_pred) ** 2)\n",
        "        ss_tot = np.sum((log_variances - np.mean(log_variances)) ** 2)\n",
        "        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0\n",
        "        \n",
        "        return {\n",
        "            'hurst_parameter': H_estimate,\n",
        "            'r_squared': r_squared,\n",
        "            'scales_used': len(valid_scales),\n",
        "            'method': 'Variance-Based'\n",
        "        }\n",
        "    \n",
        "    def _calculate_variance_at_scale(self, data, scale):\n",
        "        \"\"\"Calculate variance at a specific time scale.\"\"\"\n",
        "        n = len(data)\n",
        "        if scale >= n:\n",
        "            return 0\n",
        "        \n",
        "        # Calculate variance for non-overlapping windows\n",
        "        num_windows = n // scale\n",
        "        if num_windows < 2:\n",
        "            return 0\n",
        "        \n",
        "        variances = []\n",
        "        for i in range(num_windows):\n",
        "            start_idx = i * scale\n",
        "            end_idx = start_idx + scale\n",
        "            window_data = data[start_idx:end_idx]\n",
        "            if len(window_data) > 0:\n",
        "                variances.append(np.var(window_data))\n",
        "        \n",
        "        return np.mean(variances) if variances else 0\n",
        "\n",
        "# Test the custom estimator\n",
        "print(\"ðŸ§ª Testing Custom Variance-Based Hurst Estimator:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create estimator\n",
        "variance_estimator = VarianceBasedHurstEstimator(min_scale=10, num_scales=15)\n",
        "\n",
        "# Test on FBM data with known H\n",
        "H_test = 0.7\n",
        "fbm = FractionalBrownianMotion(H=H_test, sigma=1.0)\n",
        "test_data = fbm.generate(1000, seed=42)\n",
        "\n",
        "# Estimate Hurst parameter\n",
        "result = variance_estimator.estimate(test_data)\n",
        "\n",
        "print(f\"Test data: FBM with H = {H_test}\")\n",
        "print(f\"Estimated H: {result['hurst_parameter']:.4f}\")\n",
        "print(f\"Error: {abs(result['hurst_parameter'] - H_test):.4f}\")\n",
        "print(f\"R-squared: {result['r_squared']:.4f}\")\n",
        "print(f\"Scales used: {result['scales_used']}\")\n",
        "\n",
        "# Compare with R/S estimator\n",
        "rs_estimator = RSEstimator()\n",
        "rs_result = rs_estimator.estimate(test_data)\n",
        "rs_H = rs_result.get('hurst_parameter', rs_result.get('H', None))\n",
        "\n",
        "print(f\"\\nComparison with R/S estimator:\")\n",
        "print(f\"R/S H: {rs_H:.4f}\")\n",
        "print(f\"R/S Error: {abs(rs_H - H_test):.4f}\")\n",
        "\n",
        "print(\"\\nâœ… Custom classical estimator created and tested successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Extensibility**: LRDBenchmark is designed to be highly extensible with clear interfaces for custom components.\n",
        "\n",
        "2. **Custom Data Models**: Inherit from BaseModel and implement required methods.\n",
        "\n",
        "3. **Custom Estimators**: Follow standard interface and return consistent results.\n",
        "\n",
        "4. **Integration**: Custom components work with existing benchmark system.\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "- Parameter validation\n",
        "- Error handling  \n",
        "- Documentation\n",
        "- Testing\n",
        "- Performance consideration\n",
        "- Compatibility\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Test custom components in benchmark system\n",
        "2. Apply to real-world data\n",
        "3. Explore advanced features\n",
        "4. Share with community\n",
        "\n",
        "---\n",
        "\n",
        "**Next Notebook**: [04_comprehensive_benchmarking.ipynb](04_comprehensive_benchmarking.ipynb) - Learn how to use the comprehensive benchmarking system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
